import os
import torch
import torchvision
from torch.utils.data import random_split
import torchvision.models as models
import torch.nn as nn
import torch.nn.functional as F
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Update the path to point to your dataset directory within Google Drive
data_dir = '/content/drive/My Drive/AI/Garbage classification/Garbage classification'

torch.manual_seed(4459679)

# Check whether we have a GPU.  Use it if we do.
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')



# MNIST train and test datasets.  I'm not going to talk about these in this course.
# you should just be able to follow "recipes" online.
classes = os.listdir(data_dir)
print(classes)

from torchvision.datasets import ImageFolder
import torchvision.transforms as transforms

transformations = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])

dataset = ImageFolder(data_dir, transform = transformations)
import matplotlib.pyplot as plt
%matplotlib inline

def show_sample(img, label):
    print("Label:", dataset.classes[label], "(Class No: "+ str(label) + ")")
    plt.imshow(img.permute(1, 2, 0))


train_dataset, val_dataset, test_dataset = random_split(dataset, [1593, 176, 758])
len(train_dataset), len(val_dataset), len(test_dataset)

# MNIST train and test datasets.  I'm not going to talk about these in this course.
# However, note that I'm using a much bigger batch size at test-time.  That's
# because at training time, we have to backprop, so we have to save all the
# intermediate variables, which takes alot of memory.  We don't have to do that
# at test-time.
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=100,
                                           shuffle=True,
                                           num_workers = 2)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=150,
                                          shuffle=False,
                                          num_workers=2)

############################################
#### Tweak the network architecture !!! ####
############################################
input_size = 256*256
hidden_size = 400
num_classes = 6

model = nn.Sequential(
    nn.Conv2d(in_channels=3, out_channels=100, kernel_size=3, padding=1, stride = 1),  #Output shape is [N, 100, H, W]
    nn.ReLU(),                                                             #Output shape is [N, 100, H, W]
    nn.Conv2d(in_channels=100, out_channels=100, kernel_size=3, padding=1, stride=1), #Output shape is [N,  10, H, W]
    nn.ReLU(),                                                             #Output shape is [N, 100, H, W]
    nn.Conv2d(in_channels=100, out_channels=6, kernel_size=3, padding=1, stride=1), #Output shape is [N,  10, H, W]
    nn.AdaptiveAvgPool2d(1)                                                #output shape is [N,  10, 1, 1]; does global average pooling
).to(device)


#################################
#### Tweak the optimizer !!! ####
#################################
opt = torch.optim.Adam(model.parameters(), lr=0.01)

def train():
    # Does one training epoch (i.e. one pass over the data.)
    for images, labels in train_loader:
        # Move tensors to the configured device, and convert image to vector.
        images = images.to(device)
        labels = labels.to(device)

        # Forward pass
        logits = model(images).squeeze((-1, -2)) #output shape is [N, 10]

        # Backpropagation and optimization
        loss = nn.functional.cross_entropy(logits, labels)
        loss.backward()
        opt.step()
        opt.zero_grad()

def test(epoch):
    # Do one pass over the test data.
    # In the test phase, don't need to compute gradients (for memory efficiency)
    with torch.no_grad():
        correct = 0
        total = 0
        for images, labels in test_loader:
            #Convert image pixels to vector
            images = images.to(device)
            labels = labels.to(device)

            # Forward pass
            logits = model(images).squeeze((-1, -2))

            # Compute total correct so far
            predicted = torch.argmax(logits, -1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
        print(f'Test accuracy after {epoch+1} epochs: {100 * correct / total} %')


# Run training
for epoch in range(8):
    train()
    test(epoch)


